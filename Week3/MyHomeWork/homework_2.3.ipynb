{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 收集数据：提供的文本文件。\n",
    "2. 准备数据：将文本文件解析成词条向量。\n",
    "3. 分析数据：检查词条确保解析的正确性。\n",
    "4. 训练算法：使用trainNB0()函数。\n",
    "5. 测试算法：使用classifyNB()，并且构建一个新的测试函数来计算文档集的错误率。\n",
    "6. 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据：切分文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I hava ever laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'hava',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修正标点符号的切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入正则表达式模块\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匹配字符串\n",
    "regEx = re.compile('\\W+')\n",
    "listOfTokens = regEx.split(mySent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'hava',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将单词转换为小写，以便统一词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'hava',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据集中一封完整电子邮件的实际处理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailText = open('email/ham/6.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfTokens = regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试算法：使用朴素贝叶斯进行交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件解析及完整的垃圾邮件测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split('\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个包含在所有文档中出现的不重复词的列表\n",
    "def createVocabList(dataSet):\n",
    "    # 创建一个空集合，为了去重\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        # 创建两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    # 创建一个其中所含元素都是0的向量，进行该向量数组的初始化\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # 1或0表示词汇表中的单词在输入文档中是否出现\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word )\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "# trainMatrix-文档矩阵，trainCategory-由每篇文档类别标签所构成的向量\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 求出有侮辱性词语的文档占比\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 初始化概率，num为分子，denom为分母\n",
    "    # 如果其中一个概率值为0， 那么最后的乘积也为0，为了降低这种影响，\n",
    "    # 可以将所有词的出现数初始化为1， 并将分母初始化为2\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # p1表示侮辱性词语，p0表示正常词语\n",
    "        if trainCategory[i] == 1:\n",
    "            # 向量相加\n",
    "            # 侮辱性词语的词频数向量\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 侮辱性词语的总词数\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 对每个元素做除法，求出该词的概率即p(w|c)\n",
    "    # 解决下溢出问题：利用ln(a*b)=ln(a)+ln(b)\n",
    "    p1Vect = np.array([log(x) for x in p1Num/p1Denom])\n",
    "    p0Vect = np.array([log(x) for x in p0Num/p0Denom])\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # 元素相乘，相当于p(w0|c1)p(w1|c1)...\n",
    "    # 由于p(w)是一样的，所以在求p1的时候就不需要做除法运算了，计算p(c|w)\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    import random\n",
    "    docList = []; classList = []; fullText = []\n",
    "    for i in range(1, 26):\n",
    "        # 采用ISO-8859-1的编码读取文件\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, encoding='ISO-8859-1').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        # 标记为1\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, encoding='ISO-8859-1').read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        # 标记为0\n",
    "        classList.append(0)\n",
    "    # 创建一个包含在所有文档中出现的不重复词的列表\n",
    "    vocabList = createVocabList(docList)\n",
    "    # 由于每一类有25个文本，故一共有50个文本\n",
    "    trainingSet = range(50); testSet = []\n",
    "    # 随机取10个作为测试集，剩下的40个文本作为训练集\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(list(trainingSet)[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    # 构建初始化训练集的数据矩阵和特征值\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # 训练算法\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    # 测试算法\n",
    "    for docIndex in testSet:\n",
    "        # 将测试集数据进行向量化\n",
    "        wordVecotr = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        # 进行测试，如果发现错误，则错误数加1\n",
    "        if classifyNB(np.array(wordVecotr), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    # 打印出总的错误百分比\n",
    "    print('the error rate is: ', float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
