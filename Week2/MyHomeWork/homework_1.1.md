# 决策树分类算法的原理

    首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。决策树是通过一系列规则对数据进行分类的过程。
    决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。
    一棵决策树的生成过程主要分为以下3个部分:
    特征选择：特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。
    决策树生成： 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。 树结构来说，递归结构是最容易理解的方式。
    剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

# 优点
    计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

# 缺点
    可能会产生过度匹配问题。

# 适用数据类型
    数值型和标称型。